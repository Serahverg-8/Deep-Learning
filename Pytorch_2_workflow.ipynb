{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpOjAYIdMHJW3M55QktFfT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Serahverg-8/Deep-Learning-pytorch/blob/main/Pytorch_2_workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "gQyHfOfYdkUs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "#nn building blocks of neural nets\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. get Data ready\n",
        "2. turn into tensors\n",
        "3. build or choose training model --- > loss function and optimizer\n",
        "4.fit model to make prediction\n",
        "5.Evaluate the model\n",
        "6.Improve the model - iteration\n",
        "7. save model"
      ],
      "metadata": {
        "id": "ecm_1p1ido2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Getting data reading(turning it into tensors)"
      ],
      "metadata": {
        "id": "bOYOfWy6eK61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#creating data - linear regression. (known parameters set by us)\n",
        "#only one feature\n",
        "weight = 0.7\n",
        "bias = 0.3\n",
        "\n",
        "\n",
        "#create\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.02\n",
        "\n",
        "X = torch.arange(start,end,step).unsqueeze(dim=1)\n",
        "#usually we work with 2d so each row is one input 50 X 1 . so add 1 to dim 1\n",
        "\n",
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3wDGADrfp5b",
        "outputId": "4db9d0b7-c307-4788-a855-36c59190dad8"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wsRz0wzgIl1",
        "outputId": "405d2143-ac42-4116-b3d0-0e471e586de7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000],\n",
              "        [0.0200],\n",
              "        [0.0400],\n",
              "        [0.0600],\n",
              "        [0.0800],\n",
              "        [0.1000],\n",
              "        [0.1200],\n",
              "        [0.1400],\n",
              "        [0.1600],\n",
              "        [0.1800],\n",
              "        [0.2000],\n",
              "        [0.2200],\n",
              "        [0.2400],\n",
              "        [0.2600],\n",
              "        [0.2800],\n",
              "        [0.3000],\n",
              "        [0.3200],\n",
              "        [0.3400],\n",
              "        [0.3600],\n",
              "        [0.3800],\n",
              "        [0.4000],\n",
              "        [0.4200],\n",
              "        [0.4400],\n",
              "        [0.4600],\n",
              "        [0.4800],\n",
              "        [0.5000],\n",
              "        [0.5200],\n",
              "        [0.5400],\n",
              "        [0.5600],\n",
              "        [0.5800],\n",
              "        [0.6000],\n",
              "        [0.6200],\n",
              "        [0.6400],\n",
              "        [0.6600],\n",
              "        [0.6800],\n",
              "        [0.7000],\n",
              "        [0.7200],\n",
              "        [0.7400],\n",
              "        [0.7600],\n",
              "        [0.7800],\n",
              "        [0.8000],\n",
              "        [0.8200],\n",
              "        [0.8400],\n",
              "        [0.8600],\n",
              "        [0.8800],\n",
              "        [0.9000],\n",
              "        [0.9200],\n",
              "        [0.9400],\n",
              "        [0.9600],\n",
              "        [0.9800]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = X.mul(weight) + bias"
      ],
      "metadata": {
        "id": "GxLqzeV9gJSq"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-vEAEmDgVql",
        "outputId": "ad15919d-fd7c-4d66-aa27-e9483dc33349"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3000],\n",
              "        [0.3140],\n",
              "        [0.3280],\n",
              "        [0.3420],\n",
              "        [0.3560],\n",
              "        [0.3700],\n",
              "        [0.3840],\n",
              "        [0.3980],\n",
              "        [0.4120],\n",
              "        [0.4260],\n",
              "        [0.4400],\n",
              "        [0.4540],\n",
              "        [0.4680],\n",
              "        [0.4820],\n",
              "        [0.4960],\n",
              "        [0.5100],\n",
              "        [0.5240],\n",
              "        [0.5380],\n",
              "        [0.5520],\n",
              "        [0.5660],\n",
              "        [0.5800],\n",
              "        [0.5940],\n",
              "        [0.6080],\n",
              "        [0.6220],\n",
              "        [0.6360],\n",
              "        [0.6500],\n",
              "        [0.6640],\n",
              "        [0.6780],\n",
              "        [0.6920],\n",
              "        [0.7060],\n",
              "        [0.7200],\n",
              "        [0.7340],\n",
              "        [0.7480],\n",
              "        [0.7620],\n",
              "        [0.7760],\n",
              "        [0.7900],\n",
              "        [0.8040],\n",
              "        [0.8180],\n",
              "        [0.8320],\n",
              "        [0.8460],\n",
              "        [0.8600],\n",
              "        [0.8740],\n",
              "        [0.8880],\n",
              "        [0.9020],\n",
              "        [0.9160],\n",
              "        [0.9300],\n",
              "        [0.9440],\n",
              "        [0.9580],\n",
              "        [0.9720],\n",
              "        [0.9860]])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(X,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "ACbmG8sDgWej",
        "outputId": "38ce6c29-22c7-4b92-a378-969de92201a5"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x792611a06560>]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into train and test\n",
        "\n",
        "We do this for genaralization ( course material, practice test,  final exam)"
      ],
      "metadata": {
        "id": "-v68Wuzlgax-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_split = int(0.8 * len(X))\n",
        "\n",
        "X_train = X[:train_split]\n",
        "y_train = y[:train_split]\n",
        "\n",
        "X_test = X[train_split:]\n",
        "y_test = y[train_split:]"
      ],
      "metadata": {
        "id": "cfOtstNfhxNT"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train.shape,y_train.shape,X_test.shape,y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e48duftuiQiy",
        "outputId": "d57c7185-a820-487f-fa6b-1a2ac7adaf29"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([40, 1]) torch.Size([40, 1]) torch.Size([10, 1]) torch.Size([10, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualization functions\n",
        "\n"
      ],
      "metadata": {
        "id": "Pa_MP8o_imq_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize(X_train,y_train,X_test,y_test,predictions=None):\n",
        "  plt.figure(figsize=(10,5))\n",
        "  plt.scatter(X_train,y_train,c=\"blue\",s=4,label=\"Training Data\")\n",
        "  plt.scatter(X_test,y_test,c=\"red\",s=4,label=\"Test Data\")\n",
        "  if predictions is not  None:\n",
        "      plt.scatter(X_test,predictions,c=\"black\",s=3,label=\"Predicted\")\n",
        "  plt.legend()\n",
        "  plt.plot()"
      ],
      "metadata": {
        "id": "5YKz9FgXiu8D"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(X_train,y_train,X_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "Y98SjyV0jwd_",
        "outputId": "53e192fc-f93a-4ddb-ca90-cd82d0d8eeff"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Building Model"
      ],
      "metadata": {
        "id": "5ncr6LHpje_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####read about torch.nn.module.\n",
        "PyTorch has four (give or take) essential modules you can use to create almost any kind of neural network you can imagine.\n",
        "\n",
        "They are torch.nn, torch.optim, torch.utils.data.Dataset and torch.utils.data.DataLoader. For now, we'll focus on the first two and get to the other two later (though you may be able to guess what they do).\n",
        "\n",
        "PyTorch module\tWhat does it do?  \n",
        "\n",
        "**torch.nn**\t  \n",
        "Contains all of the building blocks for computational graphs (essentially a series of computations executed in a particular way).  \n",
        "\n",
        "**torch.nn.Parameter**:-  \n",
        "\n",
        "Stores tensors that can be used with nn.Module. If requires_grad=True gradients (used for updating model parameters via gradient descent) are calculated automatically, this is often referred to as \"autograd\".\n",
        "\n",
        "\n",
        "\n",
        "**torch.nn.Module**:-\n",
        "The base class for all neural network modules, all the building blocks for neural networks are subclasses. If you're building a neural network in PyTorch, your models should subclass nn.Module. Requires a forward() method be implemented.  \n",
        "\n",
        "**torch.optim**:-  \n",
        "Contains various optimization algorithms (these tell the model parameters stored in nn.Parameter how to best change to improve gradient descent and in turn reduce the loss).  \n",
        "\n",
        "**def forward()**:-  \n",
        "All nn.Module subclasses require a forward() method, this defines the computation that will take place on the data passed to the particular nn.Module (e.g. the linear regression formula above).\n",
        "  \n",
        "If the above sounds complex, think of like this, almost everything in a PyTorch neural network comes from torch.nn,\n",
        "\n",
        "nn.Module - contains the larger building blocks (layers).\n",
        "\n",
        "nn.Parameter contains the smaller parameters like weights and biases (put these together to make nn.Module(s)).\n",
        "\n",
        "forward() tells the larger blocks how to make calculations on inputs (tensors full of data) within nn.Module(s).\n",
        "\n",
        "torch.optim contains optimization methods on how to improve the parameters within nn.Parameter to better represent input data.\n"
      ],
      "metadata": {
        "id": "7gAAfyW2y9wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#nn.module has almost everything that builds pytorch\n",
        "\n",
        "#In the documentation It is said that the models we build shuouls also subclass nn.models. So nn.models is the superclass here.\n",
        "\n",
        "\n",
        "\n",
        "class Lin_reg(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    #initial\n",
        "\n",
        "    # we have only one parameter\n",
        "\n",
        "    #weigths\n",
        "    self.weights=nn.Parameter(torch.randn(1,requires_grad=True))\n",
        "\n",
        "    #nn.parameter is a tensor itself .when assigned with modules , they act as attribute parameter.\n",
        "    #bias\n",
        "    self.bias=nn.Parameter(torch.randn(1,requires_grad=True))\n",
        "\n",
        "\n",
        "    #Forward method is to define the computation in the model\n",
        "    #input and returns type torch.Trnsor\n",
        "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "      return self.weights*x + self.bias\n",
        "\n",
        "    # so far Our model will take the random weight and the random bias and plug it into the linear regression model equation.\n",
        "\n",
        "    #Next we need to check the error. Hence we need to optimize. torch.optim\n"
      ],
      "metadata": {
        "id": "RqoWZr4qk7zT"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MwTTvelR2QY1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets check our models parameters now"
      ],
      "metadata": {
        "id": "fbq3yK7S1V3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "model_0= Lin_reg()"
      ],
      "metadata": {
        "id": "PlJmPyTj1aZX"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apaxXPA11l0F",
        "outputId": "bfe8840e-b93f-4756-f3ea-c11a457e5502"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<generator object Module.parameters at 0x7926e3782b20>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(model_0.parameters()) #--- > these are the parameters we have set"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1C0kADX52mmp",
        "outputId": "44c40228-d620-4f16-b3d4-bfc46490d630"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([0.3367], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([0.1288], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ3D3WRR7gTg",
        "outputId": "222466e6-c2a4-47e1-ef9a-e6eb33f3ccea"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#actual values :\n",
        "\n",
        "print(weight,bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSZUSC402q_C",
        "outputId": "6199d36f-a1a9-4dc8-ab76-ca3e63f161a3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7 0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we need to get as close to these values\n"
      ],
      "metadata": {
        "id": "c4RVTeLd2_7J"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make the predictions  ---->   we use torch.inference_mode()\n"
      ],
      "metadata": {
        "id": "b66Q6ZxG3I8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "  y_predicted=model_0(X_test)\n",
        "\n",
        "y_predicted[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8nfyqm63RkN",
        "outputId": "9eb04988-11e2-482c-d071-161a52b7e186"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3982],\n",
              "        [0.4049],\n",
              "        [0.4116],\n",
              "        [0.4184],\n",
              "        [0.4251],\n",
              "        [0.4318],\n",
              "        [0.4386],\n",
              "        [0.4453],\n",
              "        [0.4520],\n",
              "        [0.4588]])"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(X_train,y_train,X_test,y_test,y_predicted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "V1Ig9i8S3nOB",
        "outputId": "511ee946-9e03-4b7f-e84e-8977f40e6f64"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "so off!!\n",
        "\n",
        "\n",
        "\n",
        "Note: in older PyTorch code you might also see torch.no_grad().\n",
        "\n",
        "```\n",
        "with torch.no_grad():\n",
        "y_preds = model_0(X_test)\n",
        "```\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "W2H7ka3m4Vuq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train Model"
      ],
      "metadata": {
        "id": "5-B9NrS25NMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WE use loss functions to optimize. Used to measure how wrong our models are from he real values. from torch.nn. It is also called cost function"
      ],
      "metadata": {
        "id": "hjd_-vJR7ACX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use any from torch.nn loss functions\n",
        "\n",
        "https://pytorch.org/docs/stable/nn.html#loss-functions"
      ],
      "metadata": {
        "id": "-oExoyiz7l7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets use L1 loss which is the Mean absolue error"
      ],
      "metadata": {
        "id": "NC7nFwHK70x1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#set up a loss function using nn.L1loss\n",
        "\n",
        "loss_function = nn.L1Loss()"
      ],
      "metadata": {
        "id": "AxlhP-fz8BZS"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#set an optimizer\n",
        "\n",
        "#torch.optim --- > has many different optimizers.\n",
        "\n",
        "\n",
        "#eg Adam SDG(stichastic gradient descent) etc  (try and see which works best)"
      ],
      "metadata": {
        "id": "uRDTnTuy8QcS"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/docs/stable/optim.html"
      ],
      "metadata": {
        "id": "DTE7bxa8DjgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(params=model_0.parameters(),lr=0.01)\n",
        "\n",
        "  #learning rate tell the step value +oer - 0.01 in this case"
      ],
      "metadata": {
        "id": "vsrrfTbkDneZ"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "params:-   \n",
        " is the target model parameters you'd like to optimize (e.g. the weights and bias values we randomly set before).  \n",
        "\n",
        "lr : -   \n",
        "is the learning rate you'd like the optimizer to update the parameters at, higher means the optimizer will try larger updates (these can sometimes be too large and the optimizer will fail to work), lower means the optimizer will try smaller updates (these can sometimes be too small and the optimizer will take too long to find the ideal values). The learning rate is considered a hyperparameter (because it's set by a machine learning engineer). Common starting values for the learning rate are 0.01, 0.001, 0.0001, however, these can also be adjusted over time (this is called learning rate scheduling).\n"
      ],
      "metadata": {
        "id": "KJHYKZSJE-EU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## #Building the training loop"
      ],
      "metadata": {
        "id": "KbnDZAQ0GNAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#one time through all the data#hyper parameters because we set them\n",
        "epochs = 1000\n"
      ],
      "metadata": {
        "id": "J9Y_ZrSSGVbj"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    ### Training\n",
        "\n",
        "    # Put model in training mode (this is the default state of a model)\n",
        "    model_0.train()\n",
        "\n",
        "    # 1. Forward pass on train data using the forward() method inside\n",
        "    y_pred = model_0(X_train)\n",
        "    # print(y_pred)\n",
        "\n",
        "    # 2. Calculate the loss (how different are our models predictions to the ground truth)\n",
        "    loss = loss_function(y_pred, y_train)\n",
        "    print(\"loss at\", epoch,loss)\n",
        "\n",
        "    # 3. Zero grad of the optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss backwards\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Progress the optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "    ### Testing\n",
        "\n",
        "    # Put the model in evaluation mode\n",
        "    model_0.eval()\n",
        "    print(model_0.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sev1daEYRQ3n",
        "outputId": "7b66ffcc-f761-4572-88cc-d921fd0cbe3c"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss at 0 tensor(0.2553, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.3601])), ('bias', tensor([0.1888]))])\n",
            "loss at 1 tensor(0.2438, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.3640])), ('bias', tensor([0.1988]))])\n",
            "loss at 2 tensor(0.2322, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.3679])), ('bias', tensor([0.2088]))])\n",
            "loss at 3 tensor(0.2207, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.3718])), ('bias', tensor([0.2188]))])\n",
            "loss at 4 tensor(0.2092, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.3757])), ('bias', tensor([0.2288]))])\n",
            "loss at 5 tensor(0.1977, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.3796])), ('bias', tensor([0.2388]))])\n",
            "loss at 6 tensor(0.1862, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.3835])), ('bias', tensor([0.2488]))])\n",
            "loss at 7 tensor(0.1746, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.3874])), ('bias', tensor([0.2588]))])\n",
            "loss at 8 tensor(0.1631, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.3913])), ('bias', tensor([0.2688]))])\n",
            "loss at 9 tensor(0.1516, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.3952])), ('bias', tensor([0.2788]))])\n",
            "loss at 10 tensor(0.1401, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.3991])), ('bias', tensor([0.2888]))])\n",
            "loss at 11 tensor(0.1285, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4030])), ('bias', tensor([0.2988]))])\n",
            "loss at 12 tensor(0.1170, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4069])), ('bias', tensor([0.3088]))])\n",
            "loss at 13 tensor(0.1061, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4108])), ('bias', tensor([0.3178]))])\n",
            "loss at 14 tensor(0.0968, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4146])), ('bias', tensor([0.3258]))])\n",
            "loss at 15 tensor(0.0891, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4184])), ('bias', tensor([0.3333]))])\n",
            "loss at 16 tensor(0.0823, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4222])), ('bias', tensor([0.3403]))])\n",
            "loss at 17 tensor(0.0764, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4258])), ('bias', tensor([0.3463]))])\n",
            "loss at 18 tensor(0.0716, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4293])), ('bias', tensor([0.3518]))])\n",
            "loss at 19 tensor(0.0675, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4328])), ('bias', tensor([0.3568]))])\n",
            "loss at 20 tensor(0.0640, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4361])), ('bias', tensor([0.3613]))])\n",
            "loss at 21 tensor(0.0610, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4394])), ('bias', tensor([0.3653]))])\n",
            "loss at 22 tensor(0.0585, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4425])), ('bias', tensor([0.3688]))])\n",
            "loss at 23 tensor(0.0564, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4455])), ('bias', tensor([0.3718]))])\n",
            "loss at 24 tensor(0.0546, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4483])), ('bias', tensor([0.3743]))])\n",
            "loss at 25 tensor(0.0531, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4512])), ('bias', tensor([0.3768]))])\n",
            "loss at 26 tensor(0.0518, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4539])), ('bias', tensor([0.3788]))])\n",
            "loss at 27 tensor(0.0507, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4564])), ('bias', tensor([0.3803]))])\n",
            "loss at 28 tensor(0.0498, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4590])), ('bias', tensor([0.3818]))])\n",
            "loss at 29 tensor(0.0490, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4615])), ('bias', tensor([0.3833]))])\n",
            "loss at 30 tensor(0.0482, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4639])), ('bias', tensor([0.3843]))])\n",
            "loss at 31 tensor(0.0475, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4662])), ('bias', tensor([0.3853]))])\n",
            "loss at 32 tensor(0.0469, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4684])), ('bias', tensor([0.3858]))])\n",
            "loss at 33 tensor(0.0464, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4706])), ('bias', tensor([0.3863]))])\n",
            "loss at 34 tensor(0.0459, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4728])), ('bias', tensor([0.3868]))])\n",
            "loss at 35 tensor(0.0454, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4748])), ('bias', tensor([0.3868]))])\n",
            "loss at 36 tensor(0.0450, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4768])), ('bias', tensor([0.3868]))])\n",
            "loss at 37 tensor(0.0446, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4788])), ('bias', tensor([0.3868]))])\n",
            "loss at 38 tensor(0.0442, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4808])), ('bias', tensor([0.3868]))])\n",
            "loss at 39 tensor(0.0438, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4828])), ('bias', tensor([0.3868]))])\n",
            "loss at 40 tensor(0.0434, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4848])), ('bias', tensor([0.3868]))])\n",
            "loss at 41 tensor(0.0431, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4866])), ('bias', tensor([0.3863]))])\n",
            "loss at 42 tensor(0.0427, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4884])), ('bias', tensor([0.3858]))])\n",
            "loss at 43 tensor(0.0424, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4902])), ('bias', tensor([0.3853]))])\n",
            "loss at 44 tensor(0.0420, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4920])), ('bias', tensor([0.3848]))])\n",
            "loss at 45 tensor(0.0417, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4938])), ('bias', tensor([0.3843]))])\n",
            "loss at 46 tensor(0.0413, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4956])), ('bias', tensor([0.3838]))])\n",
            "loss at 47 tensor(0.0410, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4974])), ('bias', tensor([0.3833]))])\n",
            "loss at 48 tensor(0.0406, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.4992])), ('bias', tensor([0.3828]))])\n",
            "loss at 49 tensor(0.0403, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5010])), ('bias', tensor([0.3823]))])\n",
            "loss at 50 tensor(0.0399, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5028])), ('bias', tensor([0.3818]))])\n",
            "loss at 51 tensor(0.0396, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5046])), ('bias', tensor([0.3813]))])\n",
            "loss at 52 tensor(0.0392, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5064])), ('bias', tensor([0.3808]))])\n",
            "loss at 53 tensor(0.0389, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5082])), ('bias', tensor([0.3803]))])\n",
            "loss at 54 tensor(0.0385, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5100])), ('bias', tensor([0.3798]))])\n",
            "loss at 55 tensor(0.0382, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5116])), ('bias', tensor([0.3788]))])\n",
            "loss at 56 tensor(0.0379, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5134])), ('bias', tensor([0.3783]))])\n",
            "loss at 57 tensor(0.0375, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5152])), ('bias', tensor([0.3778]))])\n",
            "loss at 58 tensor(0.0372, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5168])), ('bias', tensor([0.3768]))])\n",
            "loss at 59 tensor(0.0368, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5186])), ('bias', tensor([0.3763]))])\n",
            "loss at 60 tensor(0.0365, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5202])), ('bias', tensor([0.3753]))])\n",
            "loss at 61 tensor(0.0361, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5220])), ('bias', tensor([0.3748]))])\n",
            "loss at 62 tensor(0.0358, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5236])), ('bias', tensor([0.3738]))])\n",
            "loss at 63 tensor(0.0354, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5254])), ('bias', tensor([0.3733]))])\n",
            "loss at 64 tensor(0.0351, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5272])), ('bias', tensor([0.3728]))])\n",
            "loss at 65 tensor(0.0348, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5288])), ('bias', tensor([0.3718]))])\n",
            "loss at 66 tensor(0.0344, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5306])), ('bias', tensor([0.3713]))])\n",
            "loss at 67 tensor(0.0341, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5322])), ('bias', tensor([0.3703]))])\n",
            "loss at 68 tensor(0.0337, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5340])), ('bias', tensor([0.3698]))])\n",
            "loss at 69 tensor(0.0334, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5355])), ('bias', tensor([0.3688]))])\n",
            "loss at 70 tensor(0.0330, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5373])), ('bias', tensor([0.3683]))])\n",
            "loss at 71 tensor(0.0327, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5391])), ('bias', tensor([0.3678]))])\n",
            "loss at 72 tensor(0.0324, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5407])), ('bias', tensor([0.3668]))])\n",
            "loss at 73 tensor(0.0320, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5425])), ('bias', tensor([0.3663]))])\n",
            "loss at 74 tensor(0.0317, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5441])), ('bias', tensor([0.3653]))])\n",
            "loss at 75 tensor(0.0313, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5459])), ('bias', tensor([0.3648]))])\n",
            "loss at 76 tensor(0.0310, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5475])), ('bias', tensor([0.3638]))])\n",
            "loss at 77 tensor(0.0306, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5493])), ('bias', tensor([0.3633]))])\n",
            "loss at 78 tensor(0.0303, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5509])), ('bias', tensor([0.3623]))])\n",
            "loss at 79 tensor(0.0300, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5527])), ('bias', tensor([0.3618]))])\n",
            "loss at 80 tensor(0.0296, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5545])), ('bias', tensor([0.3613]))])\n",
            "loss at 81 tensor(0.0293, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5561])), ('bias', tensor([0.3603]))])\n",
            "loss at 82 tensor(0.0289, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5579])), ('bias', tensor([0.3598]))])\n",
            "loss at 83 tensor(0.0286, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5595])), ('bias', tensor([0.3588]))])\n",
            "loss at 84 tensor(0.0282, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5613])), ('bias', tensor([0.3583]))])\n",
            "loss at 85 tensor(0.0279, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5629])), ('bias', tensor([0.3573]))])\n",
            "loss at 86 tensor(0.0275, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5647])), ('bias', tensor([0.3568]))])\n",
            "loss at 87 tensor(0.0272, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5665])), ('bias', tensor([0.3563]))])\n",
            "loss at 88 tensor(0.0269, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5681])), ('bias', tensor([0.3553]))])\n",
            "loss at 89 tensor(0.0265, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5699])), ('bias', tensor([0.3548]))])\n",
            "loss at 90 tensor(0.0262, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5715])), ('bias', tensor([0.3538]))])\n",
            "loss at 91 tensor(0.0258, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5733])), ('bias', tensor([0.3533]))])\n",
            "loss at 92 tensor(0.0255, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5748])), ('bias', tensor([0.3523]))])\n",
            "loss at 93 tensor(0.0251, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5766])), ('bias', tensor([0.3518]))])\n",
            "loss at 94 tensor(0.0248, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5784])), ('bias', tensor([0.3513]))])\n",
            "loss at 95 tensor(0.0245, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5800])), ('bias', tensor([0.3503]))])\n",
            "loss at 96 tensor(0.0241, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5818])), ('bias', tensor([0.3498]))])\n",
            "loss at 97 tensor(0.0238, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5834])), ('bias', tensor([0.3488]))])\n",
            "loss at 98 tensor(0.0234, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5852])), ('bias', tensor([0.3483]))])\n",
            "loss at 99 tensor(0.0231, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5868])), ('bias', tensor([0.3473]))])\n",
            "loss at 100 tensor(0.0227, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5886])), ('bias', tensor([0.3468]))])\n",
            "loss at 101 tensor(0.0224, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5902])), ('bias', tensor([0.3458]))])\n",
            "loss at 102 tensor(0.0221, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5920])), ('bias', tensor([0.3453]))])\n",
            "loss at 103 tensor(0.0217, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5938])), ('bias', tensor([0.3448]))])\n",
            "loss at 104 tensor(0.0214, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5954])), ('bias', tensor([0.3438]))])\n",
            "loss at 105 tensor(0.0210, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5972])), ('bias', tensor([0.3433]))])\n",
            "loss at 106 tensor(0.0207, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.5988])), ('bias', tensor([0.3423]))])\n",
            "loss at 107 tensor(0.0203, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6006])), ('bias', tensor([0.3418]))])\n",
            "loss at 108 tensor(0.0200, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6022])), ('bias', tensor([0.3408]))])\n",
            "loss at 109 tensor(0.0196, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6040])), ('bias', tensor([0.3403]))])\n",
            "loss at 110 tensor(0.0193, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6058])), ('bias', tensor([0.3398]))])\n",
            "loss at 111 tensor(0.0190, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6074])), ('bias', tensor([0.3388]))])\n",
            "loss at 112 tensor(0.0186, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6092])), ('bias', tensor([0.3383]))])\n",
            "loss at 113 tensor(0.0183, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6108])), ('bias', tensor([0.3373]))])\n",
            "loss at 114 tensor(0.0179, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6126])), ('bias', tensor([0.3368]))])\n",
            "loss at 115 tensor(0.0176, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6141])), ('bias', tensor([0.3358]))])\n",
            "loss at 116 tensor(0.0172, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6159])), ('bias', tensor([0.3353]))])\n",
            "loss at 117 tensor(0.0169, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6175])), ('bias', tensor([0.3343]))])\n",
            "loss at 118 tensor(0.0166, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6193])), ('bias', tensor([0.3338]))])\n",
            "loss at 119 tensor(0.0162, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6211])), ('bias', tensor([0.3333]))])\n",
            "loss at 120 tensor(0.0159, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6227])), ('bias', tensor([0.3323]))])\n",
            "loss at 121 tensor(0.0155, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6245])), ('bias', tensor([0.3318]))])\n",
            "loss at 122 tensor(0.0152, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6261])), ('bias', tensor([0.3308]))])\n",
            "loss at 123 tensor(0.0148, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6279])), ('bias', tensor([0.3303]))])\n",
            "loss at 124 tensor(0.0145, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6295])), ('bias', tensor([0.3293]))])\n",
            "loss at 125 tensor(0.0142, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6313])), ('bias', tensor([0.3288]))])\n",
            "loss at 126 tensor(0.0138, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6331])), ('bias', tensor([0.3283]))])\n",
            "loss at 127 tensor(0.0135, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6347])), ('bias', tensor([0.3273]))])\n",
            "loss at 128 tensor(0.0131, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6365])), ('bias', tensor([0.3268]))])\n",
            "loss at 129 tensor(0.0128, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6381])), ('bias', tensor([0.3258]))])\n",
            "loss at 130 tensor(0.0124, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6399])), ('bias', tensor([0.3253]))])\n",
            "loss at 131 tensor(0.0121, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6415])), ('bias', tensor([0.3243]))])\n",
            "loss at 132 tensor(0.0118, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6433])), ('bias', tensor([0.3238]))])\n",
            "loss at 133 tensor(0.0114, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6451])), ('bias', tensor([0.3233]))])\n",
            "loss at 134 tensor(0.0111, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6467])), ('bias', tensor([0.3223]))])\n",
            "loss at 135 tensor(0.0107, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6485])), ('bias', tensor([0.3218]))])\n",
            "loss at 136 tensor(0.0104, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6501])), ('bias', tensor([0.3208]))])\n",
            "loss at 137 tensor(0.0100, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6519])), ('bias', tensor([0.3203]))])\n",
            "loss at 138 tensor(0.0097, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6534])), ('bias', tensor([0.3193]))])\n",
            "loss at 139 tensor(0.0093, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6552])), ('bias', tensor([0.3188]))])\n",
            "loss at 140 tensor(0.0090, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6568])), ('bias', tensor([0.3178]))])\n",
            "loss at 141 tensor(0.0087, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6586])), ('bias', tensor([0.3173]))])\n",
            "loss at 142 tensor(0.0083, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6604])), ('bias', tensor([0.3168]))])\n",
            "loss at 143 tensor(0.0080, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6620])), ('bias', tensor([0.3158]))])\n",
            "loss at 144 tensor(0.0076, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6638])), ('bias', tensor([0.3153]))])\n",
            "loss at 145 tensor(0.0073, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6654])), ('bias', tensor([0.3143]))])\n",
            "loss at 146 tensor(0.0069, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6672])), ('bias', tensor([0.3138]))])\n",
            "loss at 147 tensor(0.0066, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6688])), ('bias', tensor([0.3128]))])\n",
            "loss at 148 tensor(0.0063, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6706])), ('bias', tensor([0.3123]))])\n",
            "loss at 149 tensor(0.0059, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6724])), ('bias', tensor([0.3118]))])\n",
            "loss at 150 tensor(0.0056, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6740])), ('bias', tensor([0.3108]))])\n",
            "loss at 151 tensor(0.0052, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6758])), ('bias', tensor([0.3103]))])\n",
            "loss at 152 tensor(0.0049, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6774])), ('bias', tensor([0.3093]))])\n",
            "loss at 153 tensor(0.0045, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6792])), ('bias', tensor([0.3088]))])\n",
            "loss at 154 tensor(0.0042, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6808])), ('bias', tensor([0.3078]))])\n",
            "loss at 155 tensor(0.0039, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6826])), ('bias', tensor([0.3073]))])\n",
            "loss at 156 tensor(0.0035, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6844])), ('bias', tensor([0.3068]))])\n",
            "loss at 157 tensor(0.0032, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6860])), ('bias', tensor([0.3058]))])\n",
            "loss at 158 tensor(0.0028, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6878])), ('bias', tensor([0.3053]))])\n",
            "loss at 159 tensor(0.0025, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6894])), ('bias', tensor([0.3043]))])\n",
            "loss at 160 tensor(0.0021, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6912])), ('bias', tensor([0.3038]))])\n",
            "loss at 161 tensor(0.0018, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6927])), ('bias', tensor([0.3028]))])\n",
            "loss at 162 tensor(0.0015, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6947])), ('bias', tensor([0.3028]))])\n",
            "loss at 163 tensor(0.0012, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 164 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 165 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 166 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 167 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 168 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 169 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 170 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 171 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 172 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 173 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 174 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 175 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 176 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 177 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 178 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 179 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 180 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 181 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 182 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 183 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 184 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 185 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 186 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 187 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 188 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 189 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 190 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 191 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 192 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 193 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 194 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 195 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 196 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 197 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 198 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 199 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 200 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 201 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 202 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 203 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 204 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 205 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 206 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 207 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 208 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 209 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 210 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 211 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 212 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 213 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 214 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 215 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 216 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 217 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 218 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 219 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 220 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 221 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 222 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 223 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 224 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 225 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 226 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 227 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 228 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 229 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 230 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 231 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 232 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 233 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 234 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 235 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 236 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 237 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 238 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 239 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 240 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 241 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 242 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 243 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 244 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 245 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 246 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 247 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 248 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 249 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 250 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 251 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 252 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 253 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 254 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 255 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 256 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 257 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 258 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 259 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 260 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 261 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 262 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 263 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 264 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 265 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 266 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 267 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 268 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 269 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 270 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 271 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 272 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 273 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 274 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 275 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 276 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 277 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 278 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 279 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 280 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 281 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 282 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 283 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 284 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 285 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 286 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 287 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 288 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 289 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 290 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 291 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 292 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 293 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 294 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 295 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 296 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 297 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 298 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 299 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 300 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 301 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 302 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 303 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 304 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 305 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 306 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 307 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 308 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 309 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 310 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 311 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 312 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 313 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 314 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 315 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 316 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 317 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 318 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 319 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 320 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 321 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 322 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 323 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 324 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 325 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 326 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 327 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 328 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 329 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 330 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 331 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 332 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 333 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 334 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 335 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 336 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 337 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 338 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 339 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 340 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 341 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 342 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 343 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 344 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 345 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 346 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 347 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 348 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 349 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 350 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 351 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 352 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 353 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 354 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 355 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 356 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 357 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 358 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 359 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 360 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 361 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 362 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 363 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 364 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 365 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 366 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 367 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 368 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 369 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 370 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 371 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 372 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 373 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 374 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 375 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 376 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 377 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 378 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 379 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 380 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 381 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 382 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 383 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 384 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 385 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 386 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 387 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 388 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 389 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 390 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 391 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 392 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 393 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 394 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 395 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 396 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 397 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 398 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 399 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 400 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 401 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 402 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 403 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 404 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 405 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 406 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 407 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 408 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 409 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 410 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 411 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 412 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 413 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 414 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 415 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 416 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 417 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 418 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 419 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 420 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 421 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 422 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 423 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 424 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 425 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 426 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 427 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 428 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 429 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 430 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 431 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 432 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 433 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 434 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 435 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 436 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 437 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 438 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 439 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 440 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 441 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 442 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 443 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 444 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 445 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 446 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 447 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 448 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 449 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 450 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 451 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 452 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 453 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 454 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 455 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 456 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 457 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 458 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 459 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 460 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 461 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 462 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 463 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 464 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 465 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 466 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 467 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 468 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 469 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 470 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 471 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 472 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 473 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 474 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 475 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 476 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 477 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 478 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 479 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 480 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 481 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 482 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 483 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 484 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 485 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 486 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 487 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 488 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 489 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 490 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 491 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 492 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 493 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 494 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 495 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 496 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 497 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 498 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 499 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 500 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 501 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 502 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 503 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 504 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 505 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 506 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 507 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 508 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 509 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 510 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 511 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 512 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 513 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 514 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 515 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 516 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 517 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 518 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 519 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 520 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 521 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 522 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 523 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 524 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 525 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 526 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 527 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 528 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 529 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 530 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 531 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 532 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 533 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 534 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 535 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 536 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 537 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 538 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 539 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 540 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 541 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 542 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 543 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 544 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 545 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 546 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 547 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 548 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 549 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 550 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 551 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 552 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 553 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 554 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 555 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 556 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 557 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 558 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 559 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 560 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 561 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 562 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 563 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 564 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 565 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 566 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 567 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 568 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 569 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 570 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 571 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 572 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 573 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 574 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 575 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 576 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 577 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 578 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 579 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 580 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 581 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 582 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 583 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 584 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 585 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 586 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 587 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 588 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 589 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 590 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 591 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 592 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 593 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 594 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 595 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 596 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 597 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 598 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 599 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 600 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 601 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 602 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 603 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 604 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 605 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 606 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 607 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 608 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 609 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 610 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 611 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 612 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 613 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 614 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 615 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 616 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 617 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 618 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 619 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 620 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 621 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 622 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 623 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 624 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 625 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 626 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 627 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 628 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 629 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 630 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 631 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 632 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 633 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 634 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 635 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 636 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 637 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 638 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 639 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 640 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 641 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 642 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 643 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 644 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 645 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 646 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 647 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 648 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 649 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 650 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 651 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 652 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 653 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 654 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 655 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 656 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 657 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 658 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 659 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 660 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 661 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 662 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 663 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 664 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 665 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 666 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 667 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 668 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 669 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 670 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 671 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 672 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 673 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 674 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 675 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 676 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 677 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 678 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 679 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 680 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 681 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 682 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 683 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 684 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 685 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 686 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 687 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 688 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 689 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 690 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 691 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 692 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 693 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 694 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 695 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 696 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 697 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 698 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 699 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 700 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 701 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 702 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 703 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 704 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 705 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 706 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 707 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 708 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 709 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 710 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 711 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 712 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 713 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 714 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 715 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 716 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 717 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 718 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 719 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 720 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 721 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 722 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 723 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 724 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 725 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 726 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 727 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 728 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 729 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 730 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 731 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 732 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 733 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 734 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 735 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 736 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 737 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 738 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 739 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 740 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 741 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 742 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 743 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 744 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 745 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 746 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 747 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 748 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 749 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 750 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 751 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 752 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 753 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 754 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 755 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 756 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 757 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 758 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 759 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 760 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 761 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 762 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 763 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 764 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 765 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 766 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 767 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 768 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 769 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 770 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 771 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 772 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 773 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 774 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 775 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 776 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 777 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 778 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 779 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 780 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 781 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 782 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 783 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 784 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 785 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 786 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 787 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 788 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 789 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 790 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 791 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 792 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 793 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 794 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 795 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 796 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 797 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 798 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 799 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 800 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 801 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 802 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 803 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 804 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 805 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 806 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 807 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 808 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 809 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 810 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 811 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 812 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 813 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 814 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 815 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 816 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 817 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 818 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 819 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 820 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 821 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 822 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 823 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 824 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 825 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 826 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 827 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 828 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 829 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 830 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 831 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 832 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 833 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 834 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 835 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 836 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 837 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 838 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 839 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 840 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 841 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 842 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 843 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 844 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 845 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 846 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 847 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 848 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 849 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 850 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 851 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 852 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 853 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 854 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 855 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 856 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 857 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 858 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 859 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 860 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 861 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 862 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 863 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 864 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 865 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 866 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 867 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 868 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 869 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 870 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 871 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 872 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 873 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 874 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 875 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 876 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 877 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 878 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 879 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 880 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 881 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 882 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 883 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 884 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 885 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 886 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 887 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 888 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 889 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 890 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 891 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 892 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 893 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 894 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 895 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 896 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 897 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 898 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 899 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 900 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 901 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 902 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 903 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 904 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 905 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 906 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 907 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 908 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 909 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 910 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 911 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 912 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 913 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 914 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 915 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 916 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 917 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 918 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 919 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 920 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 921 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 922 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 923 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 924 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 925 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 926 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 927 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 928 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 929 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 930 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 931 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 932 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 933 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 934 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 935 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 936 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 937 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 938 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 939 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 940 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 941 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 942 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 943 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 944 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 945 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 946 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 947 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 948 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 949 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 950 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 951 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 952 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 953 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 954 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 955 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 956 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 957 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 958 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 959 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 960 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 961 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 962 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 963 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 964 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 965 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 966 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 967 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 968 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 969 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 970 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 971 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 972 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 973 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 974 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 975 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 976 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 977 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 978 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 979 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 980 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 981 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 982 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 983 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 984 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 985 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 986 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 987 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 988 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 989 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 990 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 991 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 992 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 993 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 994 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 995 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 996 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 997 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n",
            "loss at 998 tensor(0.0026, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6990])), ('bias', tensor([0.3093]))])\n",
            "loss at 999 tensor(0.0089, grad_fn=<MeanBackward0>)\n",
            "OrderedDict([('weights', tensor([0.6951])), ('bias', tensor([0.2993]))])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8LbQ72FheRM",
        "outputId": "18d0fc79-5a31-47d1-875d-c8c19304953c"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.3562])), ('bias', tensor([0.1788]))])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"actual weight and bias =\",weight , bias)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zeakgdMhvnc",
        "outputId": "b8d7dd5c-f6f5-45fe-da8d-cd0abfe87410"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "actual weight and bias = 0.7 0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "      # 1. Forward pass on test data\n",
        "      test_pred = model_0(X_test)"
      ],
      "metadata": {
        "id": "n_wWiGLIh2zX"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize(X_train,y_train,X_test,y_test,test_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "mcBbU3qBlyAG",
        "outputId": "30466654-238f-41de-86fc-57ab43477f08"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_0=Lin_reg()"
      ],
      "metadata": {
        "id": "Ww4WvWI9mxxG"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTpkC5t5mzFy",
        "outputId": "e397245d-dd7b-41aa-ed14-0aeceb6e5341"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Full function :\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Set the number of epochs (how many times the model will pass over the training data)\n",
        "epochs = 1000\n",
        "\n",
        "# Create empty loss lists to track values\n",
        "train_loss_values = []\n",
        "test_loss_values = []\n",
        "epoch_count = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    ### Training\n",
        "\n",
        "    # Put model in training mode (this is the default state of a model)\n",
        "    model_0.train()\n",
        "\n",
        "    # 1. Forward pass on train data using the forward() method inside\n",
        "    y_pred = model_0(X_train)\n",
        "    # print(y_pred)\n",
        "\n",
        "    # 2. Calculate the loss (how different are our models predictions to the ground truth)\n",
        "    loss = loss_function(y_pred, y_train)\n",
        "\n",
        "    # 3. Zero grad of the optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 4. Loss backwards\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Progress the optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "    ### Testing\n",
        "\n",
        "    # Put the model in evaluation mode\n",
        "    model_0.eval()\n",
        "\n",
        "    with torch.inference_mode():\n",
        "      # 1. Forward pass on test data\n",
        "      test_pred = model_0(X_test)\n",
        "\n",
        "      # 2. Caculate loss on test data\n",
        "      test_loss = loss_function(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type\n",
        "\n",
        "      # Print out what's happening\n",
        "      if epoch % 10 == 0:\n",
        "            epoch_count.append(epoch)\n",
        "            train_loss_values.append(loss.detach().numpy())\n",
        "            test_loss_values.append(test_loss.detach().numpy())\n",
        "            print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxr2lcHfl8-2",
        "outputId": "52d37268-a069-4475-d427-6d849fc6fc74"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 10 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 20 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 30 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 40 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 50 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 60 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 70 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 80 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 90 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 100 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 110 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 120 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 130 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 140 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 150 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 160 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 170 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 180 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 190 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 200 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 210 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 220 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 230 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 240 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 250 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 260 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 270 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 280 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 290 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 300 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 310 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 320 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 330 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 340 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 350 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 360 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 370 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 380 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 390 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 400 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 410 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 420 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 430 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 440 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 450 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 460 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 470 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 480 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 490 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 500 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 510 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 520 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 530 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 540 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 550 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 560 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 570 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 580 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 590 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 600 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 610 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 620 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 630 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 640 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 650 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 660 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 670 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 680 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 690 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 700 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 710 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 720 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 730 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 740 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 750 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 760 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 770 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 780 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 790 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 800 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 810 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 820 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 830 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 840 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 850 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 860 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 870 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 880 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 890 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 900 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 910 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 920 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 930 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 940 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 950 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 960 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 970 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 980 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n",
            "Epoch: 990 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4945361614227295 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the loss curves\n",
        "plt.plot(epoch_count, train_loss_values, label=\"Train loss\")\n",
        "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")\n",
        "plt.title(\"Training and test loss curves\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.legend();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "5DBM5OWamqEs",
        "outputId": "817e2ffd-ad73-4d51-8ac0-5313fbeac966"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HbKAsWHJnsZH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}